Training mac model...
Initialized Weights & Biases logging
Model parameters: 11,843,328
Total characters: 269,517,816
Train characters: 242,566,034 (90.0%)
Val characters: 26,951,782 (10.0%)
Vocabulary size (unique bytes): 209
Train dataset length: 236880
Val dataset length: 26320
Total characters: 269,517,816
Train characters: 242,566,034 (90.0%)
Val characters: 26,951,782 (10.0%)
Vocabulary size (unique bytes): 209
Train dataset length: 236880
Val dataset length: 26320
training loss: 5.757989883422852
validation loss: 5.559631824493408

Seed text: a || LONEOS || MAS || align=right | 1.2 km ||  |-id=939 bgcolor=#E9E9E9 | 156939 Odegard ||  ||  || March 24, 2003 || Goodricke-Pigott || V. Reddy || â || align=right | 2.0 km ||  |-id=940 bgcolor=#fefefe | 156940 ||  || â || March 25, 2003 || Anderson Mesa || LONEOS || NYS || align=right | 1.1 km ||  |-id=941 bgcolor=#E9E9E9 | 156941 ||  || â || April 2, 2003 || Haleakala || NEAT || â || align=right | 2.1 km ||  |-id=942 bgcolor=#fefefe | 156942 ||  || â || April 1, 2003 || Socorro || LINEAR || V || align=right | 1.5 km ||  |-id=943 bgcolor=#fefefe | 156943 ||  || â || April 1, 2003 || Socorro || LINEAR || NYS || align=right | 1.3 km ||  |-id=944 bgcolor=#fefefe | 156944 ||  || â || April 1, 2003 || Socorro || LINEAR || â || align=right | 1.2 km ||  |-id=945 bgcolor=#E9E9E9 | 156945 ||  || â || April 1, 2003 || Socorro || LINEAR || â || align=right | 1.5 km ||  |-id=946 bgcolor=#fefefe | 156946 ||  || â || April 1, 2003 || Socorro || LINEAR || â || align=right | 1.5 km ||  |-id=947 bg
****************************************************************************************************
Generated text: Îæ3888±¥eªH¥ $lÇ±ato¥ta`iþ&w] tAþ^¼tKtAþMÎôþ]aB ü°|¿hæ¯ Þ±8e- KM°(#t   rMìù`&`8Î|Y`heÞ4ÂthÎ æÎ²WAþ^M¥CÎ°Ò]t Ç¥ r8tAZªabl§#Cla&hã¯8B ª`Mâ°ltAþ h°l H¼I¤  K Îr P4 À MaC¤ÎÇÇÀ ÇwM *±A4M°l§tha8ã 
****************************************************************************************************
training loss: 5.563379287719727
training loss: 5.380128383636475
training loss: 5.193174839019775
training loss: 5.015426158905029
training loss: 4.807250022888184
training loss: 4.642090320587158
training loss: 4.508317470550537
training loss: 4.275203227996826
training loss: 4.051414489746094
training loss: 4.061520099639893
training loss: 3.765327215194702
training loss: 3.72750186920166
training loss: 3.7401726245880127
training loss: 3.595015525817871
training loss: 3.4099316596984863
training loss: 3.531635046005249
training loss: 3.558567523956299
training loss: 3.400855779647827
training loss: 3.3915576934814453
training loss: 3.4784998893737793
training loss: 3.465740919113159
training loss: 3.2548985481262207
training loss: 3.3898987770080566
training loss: 3.3802671432495117
training loss: 3.1681575775146484
training loss: 3.3808228969573975
training loss: 3.296504497528076
training loss: 3.214412212371826
training loss: 3.110858678817749
training loss: 3.105750799179077
training loss: 2.9825174808502197
training loss: 3.0202295780181885
training loss: 3.0528440475463867
training loss: 2.9710540771484375
training loss: 3.1212387084960938
training loss: 3.0773181915283203
training loss: 3.000479221343994
training loss: 2.9527759552001953
training loss: 2.9219415187835693
training loss: 2.8883183002471924
training loss: 2.952363967895508
training loss: 2.9597110748291016
training loss: 2.9574835300445557
training loss: 2.963263511657715
training loss: 2.9109535217285156
training loss: 2.898744821548462
training loss: 2.808732509613037
training loss: 2.9954147338867188
training loss: 2.890414237976074
training loss: 2.814070463180542
validation loss: 2.9047343730926514
training loss: 2.801316022872925
training loss: 2.7837822437286377
training loss: 2.8886983394622803
training loss: 2.7788820266723633
training loss: 2.986605644226074
training loss: 2.8337838649749756
training loss: 2.7655932903289795
training loss: 2.7837612628936768
training loss: 2.726015090942383
training loss: 2.888000965118408
training loss: 2.7834484577178955
training loss: 2.8175928592681885
training loss: 2.7880170345306396
training loss: 2.7262229919433594
training loss: 2.725147247314453
training loss: 2.74385404586792
training loss: 2.7746152877807617
training loss: 2.6982040405273438
training loss: 2.751420736312866
training loss: 2.6885547637939453
training loss: 2.760472059249878
training loss: 2.8287875652313232
training loss: 2.701941728591919
training loss: 2.878223180770874
training loss: 2.6684134006500244
training loss: 2.7619426250457764
training loss: 2.6726205348968506
training loss: 2.7054007053375244
training loss: 2.646052122116089
training loss: 2.7198832035064697
training loss: 2.7537760734558105
training loss: 2.886070728302002
training loss: 2.6662681102752686
training loss: 2.624499559402466
training loss: 2.705092668533325
training loss: 2.642033338546753
training loss: 2.6748979091644287
training loss: 2.6322314739227295
training loss: 2.6319830417633057
training loss: 2.846285104751587
training loss: 2.6509439945220947
training loss: 2.6022322177886963
training loss: 2.691657543182373
training loss: 2.64972186088562
training loss: 2.6757149696350098
training loss: 2.565157175064087
training loss: 2.568392515182495
training loss: 2.7665257453918457
training loss: 2.7038629055023193
training loss: 2.5128440856933594
validation loss: 2.916224241256714
training loss: 2.6740686893463135
training loss: 2.6577749252319336
training loss: 2.7071120738983154
training loss: 2.621762990951538
training loss: 2.6427628993988037
training loss: 2.706990957260132
training loss: 2.6603474617004395
training loss: 2.6599552631378174
training loss: 2.7295732498168945
training loss: 2.5990350246429443
training loss: 2.657996892929077
training loss: 2.6984660625457764
training loss: 2.766284227371216
training loss: 2.4874415397644043
training loss: 2.691734552383423
training loss: 2.686143159866333
training loss: 2.8220131397247314
training loss: 2.6184511184692383
training loss: 2.6231727600097656
training loss: 2.6253232955932617
training loss: 2.6560463905334473
training loss: 2.6533689498901367
training loss: 2.6750848293304443
training loss: 2.568535327911377
training loss: 2.5920934677124023
training loss: 2.5871622562408447
training loss: 2.5383517742156982
training loss: 2.5768063068389893
training loss: 2.6354355812072754
training loss: 2.6038525104522705
training loss: 2.6299471855163574
training loss: 2.550903558731079
training loss: 2.8905696868896484
training loss: 2.71743106842041
training loss: 2.6030001640319824
training loss: 2.6736807823181152
training loss: 2.7426846027374268
training loss: 2.5682644844055176
training loss: 2.5948381423950195
training loss: 2.7331695556640625
training loss: 2.6695213317871094
training loss: 2.65545916557312
training loss: 2.5770723819732666
training loss: 2.654238224029541
training loss: 2.623689651489258
training loss: 2.649512767791748
training loss: 2.5627050399780273
training loss: 2.6553452014923096
training loss: 2.6851818561553955
training loss: 2.6530601978302
validation loss: 2.663893222808838
training loss: 2.5302932262420654
training loss: 2.592945098876953
training loss: 2.625941276550293
training loss: 2.6561732292175293
training loss: 2.678957223892212
training loss: 2.732813596725464
training loss: 2.619354248046875
training loss: 2.494703769683838
training loss: 2.7037503719329834
training loss: 2.6446163654327393
training loss: 2.648468017578125
training loss: 2.5002472400665283
training loss: 2.5151264667510986
training loss: 2.631291627883911
training loss: 2.5090837478637695
training loss: 2.5957977771759033
training loss: 2.564502477645874
training loss: 2.461782693862915
training loss: 2.4936623573303223
training loss: 2.6210741996765137
training loss: 2.6282601356506348
training loss: 2.480827808380127
training loss: 2.5952892303466797
training loss: 2.578686237335205
training loss: 2.588621139526367
training loss: 2.6128687858581543
training loss: 2.6070663928985596
training loss: 2.5490224361419678
training loss: 2.613687515258789
training loss: 2.516710042953491
training loss: 2.5091300010681152
training loss: 2.618457794189453
training loss: 2.605637550354004
training loss: 2.6210904121398926
training loss: 2.4430408477783203
training loss: 2.6294195652008057
training loss: 2.6331889629364014
training loss: 2.495884895324707
training loss: 2.6838536262512207
training loss: 2.5371029376983643
training loss: 2.6275746822357178
training loss: 2.638721466064453
training loss: 2.606574535369873
training loss: 2.608489990234375
training loss: 2.621469020843506
training loss: 2.551520586013794
training loss: 2.5962955951690674
training loss: 2.6845145225524902
training loss: 2.4672670364379883
training loss: 2.5309383869171143
validation loss: 2.7037434577941895
training loss: 2.4732513427734375
training loss: 2.51123046875
training loss: 2.5643198490142822
training loss: 2.6095664501190186
training loss: 2.634040117263794
training loss: 2.618290662765503
training loss: 2.6149046421051025
training loss: 2.5736629962921143
training loss: 2.5755035877227783
training loss: 2.619521141052246
training loss: 2.6255736351013184
training loss: 2.595907211303711
training loss: 2.7003939151763916
training loss: 2.696805238723755
training loss: 2.5956928730010986
training loss: 2.5559871196746826
training loss: 2.497088670730591
training loss: 2.6438703536987305
training loss: 2.4943900108337402
training loss: 2.667431116104126
training loss: 2.6284351348876953
training loss: 2.634389638900757
training loss: 2.503450393676758
training loss: 2.65267276763916
training loss: 2.6506752967834473
training loss: 2.5678584575653076
training loss: 2.572436809539795
training loss: 2.417764663696289
training loss: 2.5596022605895996
training loss: 2.394735336303711
training loss: 2.8914036750793457
training loss: 2.602569103240967
training loss: 2.5138661861419678
training loss: 2.6363027095794678
training loss: 2.5871779918670654
training loss: 2.531491994857788
training loss: 2.6252338886260986
training loss: 2.5812928676605225
training loss: 2.570363998413086
training loss: 2.5399980545043945
training loss: 2.621217966079712
training loss: 2.6897239685058594
training loss: 2.5244245529174805
training loss: 2.4609925746917725
training loss: 2.632375717163086
training loss: 2.5590388774871826
training loss: 2.593930244445801
training loss: 2.619011640548706
training loss: 2.632235527038574
training loss: 2.4485907554626465
validation loss: 2.403395414352417

Seed text:  || align=right | 1.1 km ||  |-id=104 bgcolor=#fefefe | 158104 ||  || â || December 20, 2000 || Socorro || LINEAR || H || align=right data-sort-value="0.96" | 960 m ||  |-id=105 bgcolor=#FA8072 | 158105 ||  || â || December 24, 2000 || Haleakala || NEAT || â || align=right | 2.0 km ||  |-id=106 bgcolor=#E9E9E9 | 158106 ||  || â || December 20, 2000 || Socorro || LINEAR || EUN || align=right | 2.7 km ||  |-id=107 bgcolor=#d6d6d6 | 158107 ||  || â || December 30, 2000 || Socorro || LINEAR || â || align=right | 4.3 km ||  |-id=108 bgcolor=#E9E9E9 | 158108 ||  || â || December 30, 2000 || Socorro || LINEAR || â || align=right | 4.3 km ||  |-id=109 bgcolor=#E9E9E9 | 158109 ||  || â || December 30, 2000 || Socorro || LINEAR || DOR || align=right | 4.7 km ||  |-id=110 bgcolor=#E9E9E9 | 158110 ||  || â || January 3, 2001 || Socorro || LINEAR || â || align=right | 3.7 km ||  |-id=111 bgcolor=#E9E9E9 | 158111 ||  || â || January 4, 2001 || Anderson Mesa || LONEOS || â || align=right | 4.6 km |
****************************************************************************************************
Generated text:  |||â alis 6 |||||| Soues knd6 | â â â 1â Mogcor Jareorr 2 | an=rem | || 15 ben=rey | â 17.19) | 2 || Sowht | bbggppr=82.140 | | km ak | N 1 Jr ||| Salobght bgcight || AR â arign| Paligc
****************************************************************************************************
training loss: 2.4253053665161133
training loss: 2.615863800048828
training loss: 2.5283913612365723
training loss: 2.5683772563934326
training loss: 2.5358633995056152
training loss: 2.571974277496338
training loss: 2.585359573364258
training loss: 2.539743185043335
training loss: 2.4708938598632812
training loss: 2.6789181232452393
training loss: 2.5016963481903076
training loss: 2.626009464263916
training loss: 2.4948668479919434
training loss: 2.601685047149658
training loss: 2.6444756984710693
training loss: 2.611126184463501
training loss: 2.630376100540161
training loss: 2.5759148597717285
training loss: 2.604469060897827
training loss: 2.487841844558716
training loss: 2.705479383468628
training loss: 2.7812156677246094
training loss: 2.7368531227111816
training loss: 2.6086628437042236
training loss: 2.6043496131896973
training loss: 2.5115034580230713
training loss: 2.539686441421509
training loss: 2.4904541969299316
training loss: 2.3922955989837646
training loss: 2.6405158042907715
training loss: 2.440688371658325
training loss: 2.472073554992676
training loss: 2.6832633018493652
training loss: 2.5984978675842285
training loss: 2.6635525226593018
training loss: 2.586519479751587
training loss: 2.445941686630249
training loss: 2.6605379581451416
training loss: 2.531385660171509
training loss: 2.6335556507110596
training loss: 2.5905003547668457
training loss: 2.434662103652954
training loss: 2.6767988204956055
training loss: 2.799855947494507
training loss: 2.595794439315796
training loss: 2.6262078285217285
training loss: 2.6757850646972656
training loss: 2.511366844177246
training loss: 2.5781641006469727
training loss: 2.652128219604492
validation loss: 2.4872500896453857
training loss: 2.6160542964935303
training loss: 2.614929676055908
training loss: 2.7681491374969482
training loss: 2.4959802627563477
training loss: 2.558622360229492
training loss: 2.624377489089966
training loss: 2.60203218460083
training loss: 2.6606364250183105
training loss: 2.487795114517212
training loss: 2.667245388031006
training loss: 2.2250003814697266
training loss: 2.6595418453216553
training loss: 2.6507906913757324
training loss: 2.602898120880127
training loss: 2.5570695400238037
training loss: 2.540339231491089
training loss: 2.6058969497680664
training loss: 2.6299400329589844
training loss: 2.5016870498657227
training loss: 2.4885051250457764
training loss: 2.663222074508667
training loss: 2.5180957317352295
training loss: 2.345600128173828
training loss: 2.675851821899414
training loss: 2.6007723808288574
training loss: 2.6089370250701904
training loss: 2.5534212589263916
training loss: 2.6469340324401855
training loss: 2.5934066772460938
training loss: 2.5924928188323975
training loss: 2.520643472671509
training loss: 2.4809744358062744
training loss: 2.658951997756958
training loss: 2.6123738288879395
training loss: 2.651538372039795
training loss: 2.5728211402893066
training loss: 2.5439558029174805
training loss: 2.5181236267089844
training loss: 2.537013053894043
training loss: 2.5748238563537598
training loss: 2.416154623031616
training loss: 2.608682632446289
training loss: 2.5302212238311768
training loss: 2.6090877056121826
training loss: 2.5712149143218994
training loss: 2.5960447788238525
training loss: 2.4616549015045166
training loss: 2.440772533416748
training loss: 2.4908804893493652
training loss: 2.4063124656677246
validation loss: 2.5124104022979736
training loss: 2.5408623218536377
training loss: 2.5259459018707275
training loss: 2.5175485610961914
training loss: 2.667991876602173
training loss: 2.435370922088623
training loss: 2.5087718963623047
training loss: 2.501211404800415
training loss: 2.6875925064086914
training loss: 2.4115195274353027
training loss: 2.502673864364624
training loss: 2.6655545234680176
training loss: 2.6335372924804688
training loss: 2.6500089168548584
training loss: 2.493476152420044
training loss: 2.4660873413085938
training loss: 2.613218069076538
training loss: 2.3747339248657227
training loss: 2.565901756286621
training loss: 2.5900092124938965
training loss: 2.603376626968384
training loss: 2.5592355728149414
training loss: 2.4958789348602295
training loss: 2.604477643966675
training loss: 2.348336696624756
training loss: 2.547111749649048
training loss: 2.587068796157837
training loss: 2.6664140224456787
training loss: 2.6251628398895264
training loss: 2.427476406097412
training loss: 2.6751203536987305
training loss: 2.610795497894287
training loss: 2.4599366188049316
training loss: 2.6170778274536133
training loss: 2.5156967639923096
training loss: 2.5232577323913574
training loss: 2.5468146800994873
training loss: 2.627448797225952
training loss: 2.4762117862701416
training loss: 2.6080236434936523
training loss: 2.5549633502960205
training loss: 2.5174171924591064
training loss: 2.4491829872131348
training loss: 2.6088128089904785
training loss: 2.607848644256592
training loss: 2.5624256134033203
training loss: 2.440707206726074
training loss: 2.476304531097412
training loss: 2.4823269844055176
training loss: 2.3473098278045654
training loss: 2.561162233352661
validation loss: 2.443565845489502
training loss: 2.697789430618286
training loss: 2.551327705383301
training loss: 2.569401741027832
training loss: 2.560472011566162
training loss: 2.517585277557373
training loss: 2.662213087081909
training loss: 2.5816421508789062
training loss: 2.622405529022217
training loss: 2.785727024078369
training loss: 2.5944740772247314
training loss: 2.572554111480713
training loss: 2.3775925636291504
training loss: 2.4691102504730225
training loss: 2.6832633018493652
training loss: 2.641784906387329
training loss: 2.5153470039367676
training loss: 2.579252243041992
training loss: 2.476391077041626
training loss: 2.5460524559020996
training loss: 2.62644624710083
training loss: 2.5579123497009277
training loss: 2.489074230194092
training loss: 2.469573974609375
training loss: 2.2185869216918945
training loss: 2.3473570346832275
training loss: 2.554145574569702
training loss: 2.5489704608917236
training loss: 2.5500032901763916
training loss: 2.6262364387512207
training loss: 2.6516218185424805
training loss: 2.532606363296509
training loss: 2.558645248413086
training loss: 2.6933255195617676
training loss: 2.4416863918304443
training loss: 2.593355178833008
training loss: 2.1819934844970703
training loss: 2.526740550994873
training loss: 2.456725597381592
training loss: 2.617257595062256
training loss: 2.6136622428894043
training loss: 2.590463876724243
training loss: 2.326408863067627
training loss: 2.6389942169189453
training loss: 2.500343084335327
training loss: 2.487584352493286
training loss: 2.580333948135376
training loss: 2.52486515045166
training loss: 2.4213874340057373
training loss: 2.495887279510498
training loss: 2.373460054397583
validation loss: 2.3328163623809814
training loss: 2.5367932319641113
training loss: 2.735016107559204
training loss: 2.408372163772583
training loss: 2.5082404613494873
training loss: 2.406374931335449
training loss: 2.6450159549713135
training loss: 2.620610475540161
training loss: 2.5739285945892334
training loss: 2.607321262359619
training loss: 2.6475696563720703
training loss: 2.608048439025879
training loss: 2.4984734058380127
training loss: 2.598301649093628
training loss: 2.6360557079315186
training loss: 2.607867956161499
training loss: 2.5153050422668457
training loss: 2.63773775100708
training loss: 2.629809617996216
training loss: 2.4601831436157227
training loss: 2.5287420749664307
training loss: 2.505499839782715
training loss: 2.5287678241729736
training loss: 2.6270761489868164
training loss: 2.61141037940979
training loss: 2.51457142829895
training loss: 2.6269032955169678
training loss: 2.515583038330078
training loss: 2.571950674057007
training loss: 2.4692037105560303
training loss: 2.652967929840088
training loss: 2.5921072959899902
training loss: 2.4877116680145264
training loss: 2.650458812713623
training loss: 2.382802963256836
training loss: 2.4982974529266357
training loss: 2.3623270988464355
training loss: 2.634958505630493
training loss: 2.584630012512207
training loss: 2.594665050506592
training loss: 2.527771472930908
training loss: 2.678511142730713
training loss: 2.50213360786438
training loss: 2.517230272293091
training loss: 2.6447224617004395
training loss: 2.5026392936706543
training loss: 2.5715792179107666
training loss: 2.66086483001709
training loss: 2.594031810760498
training loss: 2.5971481800079346
training loss: 2.2531559467315674
validation loss: 2.509999990463257

Seed text: 1 || Haleakala || NEAT || EOS || align=right | 4.1 km ||  |-id=146 bgcolor=#d6d6d6 | 158146 ||  || â || March 29, 2001 || Anderson Mesa || LONEOS || â || align=right | 5.1 km ||  |-id=147 bgcolor=#d6d6d6 | 158147 ||  || â || March 29, 2001 || Socorro || LINEAR || â || align=right | 5.7 km ||  |-id=148 bgcolor=#d6d6d6 | 158148 ||  || â || March 24, 2001 || Anderson Mesa || LONEOS || â || align=right | 5.5 km ||  |-id=149 bgcolor=#d6d6d6 | 158149 ||  || â || March 26, 2001 || Socorro || LINEAR || HYG || align=right | 4.6 km ||  |-id=150 bgcolor=#d6d6d6 | 158150 ||  || â || April 15, 2001 || Socorro || LINEAR || TIR || align=right | 5.8 km ||  |-id=151 bgcolor=#d6d6d6 | 158151 ||  || â || April 18, 2001 || Socorro || LINEAR || VER || align=right | 7.0 km ||  |-id=152 bgcolor=#d6d6d6 | 158152 ||  || â || April 22, 2001 || Haleakala || NEAT || 7:4* || align=right | 7.1 km ||  |-id=153 bgcolor=#FA8072 | 158153 ||  || â || May 24, 2001 || Socorro || LINEAR || â || align=right data-sort-value=
****************************************************************************************************
Generated text: 843 ||||| Fid=#d=- || ||| LINaligcht 13 |||||| ||| 23185 km |||| S|  ||| ||||| 2  | | | 2-id645 KEAG ||||||| ||| |ambght â â | km  || km |||||||| || || || Magct |||-igckm-LE9, bgco | | â 19 | ||
****************************************************************************************************
training loss: 2.59830379486084
training loss: 2.6528563499450684
training loss: 2.5464935302734375
training loss: 2.6333117485046387
training loss: 2.3613510131835938
training loss: 2.475374460220337
training loss: 2.5705528259277344
training loss: 2.4852449893951416
training loss: 2.4441986083984375
training loss: 2.467346429824829
training loss: 2.5870585441589355
training loss: 2.4546475410461426
training loss: 2.440662384033203
training loss: 2.5236568450927734
training loss: 2.532763957977295
training loss: 2.570868492126465
training loss: 2.622764825820923
training loss: 2.6084022521972656
training loss: 2.6240811347961426
training loss: 2.525362253189087
training loss: 2.42362380027771
training loss: 2.627155065536499
training loss: 2.370877504348755
training loss: 2.5027964115142822
training loss: 2.60622239112854
training loss: 2.520939588546753
training loss: 2.527177572250366
training loss: 2.496032476425171
training loss: 2.400869846343994
training loss: 2.6643617153167725
training loss: 2.6546196937561035
training loss: 2.5316109657287598
training loss: 2.5758941173553467
training loss: 2.3981852531433105
training loss: 2.532594680786133
training loss: 2.549659013748169
training loss: 2.619316577911377
training loss: 2.572141408920288
training loss: 2.3020687103271484
training loss: 2.4837563037872314
training loss: 2.3106753826141357
training loss: 2.5677242279052734
training loss: 2.5805394649505615
training loss: 2.303657293319702
training loss: 2.4356882572174072
training loss: 2.572329521179199
training loss: 2.4835622310638428
training loss: 2.5405471324920654
training loss: 2.495215892791748
training loss: 2.614579439163208
validation loss: 2.6060822010040283
training loss: 2.519573211669922
training loss: 2.5920984745025635
training loss: 2.574707269668579
training loss: 2.582632541656494
training loss: 2.623365640640259
training loss: 2.612095355987549
training loss: 2.5515222549438477
training loss: 2.661409616470337
training loss: 2.5319037437438965
training loss: 2.546027660369873
training loss: 2.576673746109009
training loss: 2.6115708351135254
training loss: 2.356473445892334
training loss: 2.5110533237457275
training loss: 2.6593735218048096
training loss: 2.4693241119384766
training loss: 2.421365737915039
training loss: 2.3915860652923584
training loss: 2.599987506866455
training loss: 2.5863184928894043
training loss: 2.515706777572632
training loss: 2.478922128677368
training loss: 2.50286602973938
training loss: 2.5461435317993164
training loss: 2.49397611618042
training loss: 2.6118950843811035
training loss: 2.5467169284820557
training loss: 2.5529592037200928
training loss: 2.6303703784942627
training loss: 2.793797731399536
training loss: 2.6387059688568115
training loss: 2.6133739948272705
training loss: 2.5941848754882812
training loss: 2.48239803314209
training loss: 2.577390670776367
training loss: 2.3190462589263916
training loss: 2.4492430686950684
training loss: 2.6877498626708984
training loss: 2.6155848503112793
training loss: 2.5757763385772705
training loss: 2.6737422943115234
training loss: 2.6110260486602783
training loss: 2.5145046710968018
training loss: 2.678269624710083
training loss: 2.5121734142303467
training loss: 2.5417473316192627
training loss: 2.5088396072387695
training loss: 2.586941957473755
training loss: 2.5910048484802246
training loss: 2.6279828548431396
validation loss: 2.522186040878296
training loss: 2.2090461254119873
training loss: 2.4147746562957764
training loss: 2.5552313327789307
training loss: 2.408489227294922
training loss: 2.677717685699463
training loss: 2.4686968326568604
training loss: 2.4303953647613525
training loss: 2.6126339435577393
training loss: 2.559450626373291
training loss: 2.579376220703125
training loss: 2.5112838745117188
training loss: 2.52382755279541
training loss: 2.6731560230255127
training loss: 2.4372425079345703
training loss: 2.501244068145752
training loss: 2.611480474472046
training loss: 2.5970630645751953
training loss: 2.5725276470184326
training loss: 2.6672563552856445
training loss: 2.5800209045410156
training loss: 2.6451380252838135
training loss: 2.6349499225616455
training loss: 2.546782970428467
training loss: 2.5857129096984863
training loss: 2.5300099849700928
training loss: 2.72310209274292
training loss: 2.6288158893585205
training loss: 2.4654860496520996
training loss: 2.594123363494873
training loss: 2.4787235260009766
training loss: 2.595031261444092
training loss: 2.537212610244751
training loss: 2.4803617000579834
training loss: 2.6193432807922363
training loss: 2.525879383087158
training loss: 2.5575060844421387
training loss: 2.410553216934204
training loss: 2.586892604827881
training loss: 2.585562229156494
training loss: 2.6284737586975098
training loss: 2.4530389308929443
training loss: 2.5668017864227295
training loss: 2.4577434062957764
training loss: 2.4651882648468018
training loss: 2.4905107021331787
training loss: 2.6513843536376953
training loss: 2.6298840045928955
training loss: 2.680567979812622
training loss: 2.3934178352355957
training loss: 2.6531715393066406
validation loss: 2.5008742809295654
training loss: 2.46783709526062
training loss: 2.4940319061279297
training loss: 2.6023998260498047
training loss: 2.60050630569458
training loss: 2.5491013526916504
training loss: 2.4473562240600586
training loss: 2.577113151550293
training loss: 2.5530803203582764
training loss: 2.4640982151031494
training loss: 2.493892192840576
training loss: 2.4864253997802734
training loss: 2.565455436706543
training loss: 2.346311092376709
training loss: 2.560863494873047
training loss: 2.4933431148529053
training loss: 2.5937647819519043
training loss: 2.5828540325164795
training loss: 2.597311019897461
training loss: 2.6131298542022705
training loss: 2.229722261428833
training loss: 2.482544183731079
training loss: 2.620448589324951
training loss: 2.6538660526275635
training loss: 2.436434268951416
training loss: 2.6331801414489746
training loss: 2.4796712398529053
training loss: 2.5076656341552734
training loss: 2.5216193199157715
training loss: 2.569868326187134
training loss: 2.578108310699463
training loss: 2.467773675918579
training loss: 2.5859639644622803
training loss: 2.4481589794158936
training loss: 2.4791769981384277
training loss: 2.6455228328704834
training loss: 2.5426008701324463
training loss: 2.4878249168395996
training loss: 2.452735185623169
training loss: 2.5689635276794434
training loss: 2.4475669860839844
training loss: 2.584303855895996
training loss: 2.4944827556610107
training loss: 2.4936511516571045
training loss: 2.4950339794158936
training loss: 2.439840078353882
training loss: 2.587010622024536
training loss: 2.443333625793457
training loss: 2.355210065841675
training loss: 2.5513339042663574
training loss: 2.610137939453125
validation loss: 2.4931068420410156
training loss: 2.4271161556243896
training loss: 2.5771536827087402
training loss: 2.4444754123687744
training loss: 2.499741554260254
training loss: 2.516176462173462
training loss: 2.4913454055786133
training loss: 2.6095404624938965
training loss: 2.471863031387329
training loss: 2.611849069595337
training loss: 2.501970052719116
training loss: 2.432385206222534
training loss: 2.377995491027832
training loss: 2.5510194301605225
training loss: 2.446650266647339
training loss: 2.3655476570129395
training loss: 2.6216907501220703
training loss: 2.5487513542175293
training loss: 2.530897855758667
training loss: 2.480018377304077
training loss: 2.5879437923431396
training loss: 2.6461734771728516
training loss: 2.534783124923706
training loss: 2.5277481079101562
training loss: 2.355928421020508
training loss: 2.464076519012451
training loss: 2.6422359943389893
training loss: 2.5957186222076416
training loss: 2.5247819423675537
training loss: 2.6866455078125
training loss: 2.542428731918335
training loss: 2.574396848678589
training loss: 2.6561672687530518
training loss: 2.57328724861145
training loss: 2.4453022480010986
training loss: 2.3431015014648438
training loss: 2.3671231269836426
training loss: 2.5060508251190186
training loss: 2.625244379043579
training loss: 2.6238901615142822
training loss: 2.5183849334716797
training loss: 2.604570150375366
training loss: 2.5725739002227783
training loss: 2.5305256843566895
training loss: 2.4543259143829346
training loss: 2.6393513679504395
training loss: 2.5015878677368164
training loss: 2.557938575744629
training loss: 2.431612491607666
training loss: 2.479461431503296
training loss: 2.4375977516174316
validation loss: 2.4752748012542725

Seed text: gn=right | 1.1 km ||  |-id=613 bgcolor=#fefefe | 158613 ||  || â || January 5, 2003 || Socorro || LINEAR || â || align=right | 1.7 km ||  |-id=614 bgcolor=#fefefe | 158614 ||  || â || January 5, 2003 || Socorro || LINEAR || â || align=right | 1.5 km ||  |-id=615 bgcolor=#fefefe | 158615 ||  || â || January 5, 2003 || Socorro || LINEAR || â || align=right | 1.4 km ||  |-id=616 bgcolor=#fefefe | 158616 ||  || â || January 5, 2003 || Socorro || LINEAR || NYS || align=right | 1.1 km ||  |-id=617 bgcolor=#fefefe | 158617 ||  || â || January 5, 2003 || Socorro || LINEAR || â || align=right | 1.2 km ||  |-id=618 bgcolor=#fefefe | 158618 ||  || â || January 10, 2003 || Socorro || LINEAR || â || align=right | 1.2 km ||  |-id=619 bgcolor=#fefefe | 158619 ||  || â || January 12, 2003 || Kitt Peak || Spacewatch || V || align=right | 1.1 km ||  |-id=620 bgcolor=#fefefe | 158620 ||  || â || January 7, 2003 || Bergisch Gladbach || W. Bickel || â || align=right | 1.5 km ||  |-id=621 bgcolor=#fefef
****************************************************************************************************
Generated text: e km | 1 | Solightesst  || ||||  km NE9EAR | |-ignign=#d6| 2 | Aur=#ferigcombght ||-igcor Austr=ro at | ||||  || bght | Meecocololignuefefefeer | 181 | ||-i â9 | kmbght | ||| E9EOR 2 ||| |-iembgcoli
****************************************************************************************************
training loss: 2.460808515548706
training loss: 2.6259586811065674
training loss: 2.5441505908966064
training loss: 2.654999017715454
training loss: 2.641759157180786
training loss: 2.4969544410705566
training loss: 2.570295572280884
training loss: 2.579759120941162
training loss: 2.454385280609131
training loss: 2.5822057723999023
training loss: 2.5438497066497803
training loss: 2.434156656265259
training loss: 2.4908363819122314
training loss: 2.3180010318756104
training loss: 2.510834217071533
training loss: 2.576711654663086
training loss: 2.5756113529205322
training loss: 2.6165075302124023
training loss: 2.5635581016540527
training loss: 2.5871927738189697
training loss: 2.650556802749634
training loss: 2.5622169971466064
training loss: 2.61199951171875
training loss: 2.481764316558838
training loss: 2.5870301723480225
training loss: 2.7003467082977295
training loss: 2.601919651031494
training loss: 2.4444618225097656
training loss: 2.616270065307617
training loss: 2.604285955429077
training loss: 2.5318732261657715
training loss: 2.4741034507751465
training loss: 2.4385504722595215
training loss: 2.460453510284424
training loss: 2.0927436351776123
training loss: 2.6358354091644287
training loss: 2.5518486499786377
training loss: 2.586803436279297
training loss: 2.5345005989074707
training loss: 2.5780580043792725
training loss: 2.618788242340088
training loss: 2.2031779289245605
training loss: 2.6235697269439697
training loss: 2.1115269660949707
training loss: 2.4726061820983887
training loss: 2.34194278717041
training loss: 2.4542582035064697
training loss: 2.1621127128601074
training loss: 2.251788377761841
training loss: 2.5940027236938477
validation loss: 2.5730295181274414
training loss: 2.4984140396118164
training loss: 2.5700953006744385
training loss: 2.6050608158111572
training loss: 2.4094502925872803
training loss: 2.344670295715332
training loss: 2.7057018280029297
training loss: 2.4426372051239014
training loss: 2.5652384757995605
training loss: 2.5197031497955322
training loss: 2.490996837615967
training loss: 2.5531277656555176
training loss: 2.6399147510528564
training loss: 2.6150100231170654
training loss: 2.4102389812469482
training loss: 2.4886837005615234
training loss: 2.4894165992736816
training loss: 2.4167704582214355
training loss: 2.389974594116211
training loss: 2.5893771648406982
training loss: 2.6056461334228516
training loss: 2.607450008392334
training loss: 2.521286964416504
training loss: 2.4626123905181885
training loss: 2.49047589302063
training loss: 2.557717800140381
training loss: 2.506289482116699
training loss: 2.6038854122161865
training loss: 2.631955146789551
training loss: 2.501709461212158
training loss: 2.684619188308716
training loss: 2.51918625831604
training loss: 2.410351514816284
training loss: 2.5161404609680176
training loss: 2.466078519821167
training loss: 2.565053939819336
training loss: 2.519352436065674
training loss: 2.5735654830932617
training loss: 2.4434022903442383
training loss: 2.3209543228149414
training loss: 2.3866968154907227
training loss: 2.567260980606079
training loss: 2.4811534881591797
training loss: 2.460714817047119
training loss: 2.6207363605499268
training loss: 2.563861846923828
training loss: 2.470496892929077
training loss: 2.5950376987457275
training loss: 2.4918136596679688
training loss: 2.5964977741241455
training loss: 2.5967423915863037
validation loss: 2.6366021633148193
training loss: 2.519460678100586
training loss: 2.575406074523926
training loss: 2.555070638656616
training loss: 2.45926833152771
training loss: 2.796597719192505
training loss: 2.5523149967193604
training loss: 2.829167366027832
training loss: 2.4726691246032715
training loss: 2.464648485183716
training loss: 2.605323314666748
training loss: 2.5677127838134766
training loss: 2.4976589679718018
training loss: 2.4569501876831055
training loss: 2.6162772178649902
training loss: 2.533902883529663
training loss: 2.591268539428711
training loss: 2.5463156700134277
training loss: 2.4912924766540527
training loss: 2.5998330116271973
training loss: 2.5626912117004395
training loss: 2.6157171726226807
training loss: 2.6013381481170654
training loss: 2.6325700283050537
training loss: 2.597155809402466
training loss: 2.476655960083008
training loss: 2.4932994842529297
training loss: 2.5055489540100098
training loss: 2.46976375579834
training loss: 2.59307861328125
training loss: 2.5655219554901123
training loss: 2.4741203784942627
training loss: 2.5846619606018066
training loss: 2.5669381618499756
training loss: 2.606257200241089
training loss: 2.5684704780578613
training loss: 2.5472068786621094
training loss: 2.6329879760742188
training loss: 2.674961566925049
training loss: 2.4290730953216553
training loss: 2.4206807613372803
training loss: 2.5871050357818604
training loss: 2.6479625701904297
training loss: 2.619912624359131
training loss: 2.5324320793151855
training loss: 2.4978139400482178
training loss: 2.6281728744506836
training loss: 2.3327810764312744
training loss: 2.546588659286499
training loss: 2.4374537467956543
training loss: 2.626040458679199
validation loss: 2.4262125492095947
training loss: 2.3167507648468018
training loss: 2.6300299167633057
training loss: 2.465228319168091
training loss: 2.4696922302246094
training loss: 2.4918136596679688
training loss: 2.5186002254486084
training loss: 2.520658016204834
training loss: 2.41392183303833
training loss: 2.589608907699585
training loss: 2.4265549182891846
training loss: 2.5249791145324707
training loss: 2.4174702167510986
training loss: 2.419635534286499
training loss: 2.5913407802581787
training loss: 2.4819507598876953
training loss: 2.5419914722442627
training loss: 2.5374457836151123
training loss: 2.589675188064575
training loss: 2.495561361312866
training loss: 2.4884791374206543
training loss: 2.6165881156921387
training loss: 2.4513580799102783
training loss: 2.4709677696228027
training loss: 2.448016405105591
training loss: 2.509150505065918
training loss: 2.5647568702697754
training loss: 2.4526050090789795
training loss: 2.3119091987609863
training loss: 2.5063681602478027
training loss: 2.512392044067383
training loss: 2.5669193267822266
training loss: 2.6806740760803223
training loss: 2.6820030212402344
training loss: 2.5396032333374023
training loss: 2.5493502616882324
training loss: 2.60941219329834
training loss: 2.6545896530151367
training loss: 2.5955164432525635
training loss: 2.5314414501190186
training loss: 2.558793783187866
training loss: 2.7604708671569824
training loss: 2.4950690269470215
training loss: 2.491346597671509
training loss: 2.6045775413513184
training loss: 2.163686752319336
training loss: 2.6597933769226074
training loss: 2.603513479232788
training loss: 2.5914652347564697
training loss: 2.590315103530884
training loss: 2.4126195907592773
validation loss: 2.5101404190063477
training loss: 2.55592942237854
training loss: 2.4555764198303223
training loss: 2.6066951751708984
training loss: 2.5646798610687256
training loss: 2.5769572257995605
training loss: 2.4884631633758545
training loss: 2.5861752033233643
training loss: 2.3437204360961914
training loss: 2.623795747756958
training loss: 2.4858036041259766
training loss: 2.6810736656188965
training loss: 2.5144636631011963
training loss: 2.5151419639587402
training loss: 2.617447853088379
training loss: 2.4507195949554443
training loss: 2.546133041381836
training loss: 2.421051502227783
training loss: 2.5648396015167236
training loss: 2.600358009338379
training loss: 2.6625258922576904
training loss: 2.5272560119628906
training loss: 2.552995443344116
training loss: 2.5530483722686768
training loss: 2.4271388053894043
training loss: 2.509716033935547
training loss: 2.537247657775879
training loss: 2.4470415115356445
training loss: 2.4130091667175293
training loss: 2.581510543823242
training loss: 2.5621836185455322
training loss: 2.538287878036499
training loss: 2.433720350265503
training loss: 2.568570375442505
training loss: 2.548558473587036
training loss: 2.561950445175171
training loss: 2.576732873916626
training loss: 2.5368499755859375
training loss: 2.538893699645996
training loss: 2.6458280086517334
training loss: 2.5968644618988037
training loss: 2.572026252746582
training loss: 2.6007707118988037
training loss: 2.252408266067505
training loss: 2.6180827617645264
training loss: 2.4951603412628174
training loss: 2.6072216033935547
training loss: 2.491654396057129
training loss: 2.5573763847351074
training loss: 2.501574754714966
training loss: 2.565030813217163
validation loss: 2.3866167068481445

Seed text: AR || â || align=right | 2.8 km ||  |-id=444 bgcolor=#E9E9E9 | 158444 ||  || â || February 7, 2002 || Socorro || LINEAR || â || align=right | 2.4 km ||  |-id=445 bgcolor=#E9E9E9 | 158445 ||  || â || February 7, 2002 || Socorro || LINEAR || â || align=right | 3.6 km ||  |-id=446 bgcolor=#E9E9E9 | 158446 ||  || â || February 7, 2002 || Socorro || LINEAR || HEN || align=right | 2.2 km ||  |-id=447 bgcolor=#E9E9E9 | 158447 ||  || â || February 7, 2002 || Socorro || LINEAR || â || align=right | 3.7 km ||  |-id=448 bgcolor=#E9E9E9 | 158448 ||  || â || February 7, 2002 || Socorro || LINEAR || â || align=right | 3.9 km ||  |-id=449 bgcolor=#E9E9E9 | 158449 ||  || â || February 7, 2002 || Socorro || LINEAR || â || align=right | 1.7 km ||  |-id=450 bgcolor=#fefefe | 158450 ||  || â || February 7, 2002 || Socorro || LINEAR || NYS || align=right | 1.4 km ||  |-id=451 bgcolor=#E9E9E9 | 158451 ||  || â || February 7, 2002 || Socorro || LINEAR || â || align=right | 1.4 km ||  |-id=452 bgcolor=#
****************************************************************************************************
Generated text: Fa 10t 1200089 AR 12988, 1 agnor=#E7 | | | â 1119EFLeolocoligcowalid6 â || â | ||  20008.34, 8 bgcht  AR Sefeht Melin=r=#E9EAR 73 1, bgcefe || â  | â | | 1200006 |-igcor=7 || 2  | LINE9 alid
****************************************************************************************************
training loss: 2.384265184402466
training loss: 2.4440834522247314
training loss: 2.6217994689941406
training loss: 2.576828718185425
training loss: 2.441863775253296
training loss: 2.5987274646759033
training loss: 2.5151071548461914
training loss: 2.4922313690185547
training loss: 2.6570138931274414
training loss: 2.348167657852173
training loss: 2.3077878952026367
training loss: 2.419782876968384
training loss: 2.4923653602600098
training loss: 2.5807418823242188
training loss: 2.4400250911712646
training loss: 2.450787305831909
training loss: 2.342379093170166
training loss: 2.571897268295288
training loss: 2.5562307834625244
training loss: 2.587777853012085
training loss: 2.613478899002075
training loss: 2.5122852325439453
training loss: 2.6108717918395996
training loss: 2.5840556621551514
training loss: 2.580237627029419
training loss: 2.617215156555176
training loss: 2.5951693058013916
training loss: 2.544992446899414
training loss: 2.510847330093384
training loss: 2.5251424312591553
training loss: 2.378326892852783
training loss: 2.237840175628662
training loss: 2.632413387298584
training loss: 2.562861680984497
training loss: 2.4272634983062744
training loss: 2.588428258895874
training loss: 2.362637996673584
training loss: 2.536393880844116
training loss: 2.6220157146453857
training loss: 2.431422472000122
training loss: 2.568260908126831
training loss: 2.56589674949646
training loss: 2.3429789543151855
training loss: 2.4171767234802246
training loss: 2.579746961593628
training loss: 2.350383758544922
training loss: 2.3957269191741943
training loss: 2.5974011421203613
training loss: 2.481907606124878
training loss: 2.6105213165283203
validation loss: 2.523594617843628
training loss: 2.507722854614258
training loss: 2.468407154083252
training loss: 2.470057249069214
training loss: 2.534271478652954
training loss: 2.592829942703247
training loss: 2.472987651824951
training loss: 2.554795980453491
training loss: 2.6675779819488525
training loss: 2.3185901641845703
training loss: 2.6234700679779053
training loss: 2.4108800888061523
training loss: 2.4394288063049316
training loss: 2.796708583831787
training loss: 2.4165396690368652
training loss: 2.579430341720581
training loss: 2.409806251525879
training loss: 2.446645975112915
training loss: 2.542238473892212
training loss: 2.664667844772339
training loss: 2.4049065113067627
training loss: 2.3342268466949463
training loss: 2.449078321456909
training loss: 2.5614659786224365
training loss: 2.3768274784088135
training loss: 2.593452215194702
training loss: 2.356149196624756
training loss: 2.580334424972534
training loss: 2.6135294437408447
training loss: 2.555464744567871
training loss: 2.6450977325439453
training loss: 2.4874911308288574
training loss: 2.548238754272461
training loss: 2.57590389251709
training loss: 2.5166473388671875
training loss: 2.5499589443206787
training loss: 2.5978612899780273
training loss: 2.472374439239502
training loss: 2.6023988723754883
training loss: 2.4219486713409424
training loss: 2.557248592376709
training loss: 2.6269843578338623
training loss: 2.524416446685791
training loss: 2.5888161659240723
training loss: 2.5346016883850098
training loss: 2.3082473278045654
training loss: 2.3137638568878174
training loss: 2.420966148376465
training loss: 2.5764710903167725
training loss: 2.6912930011749268
training loss: 2.5932626724243164
validation loss: 2.625941038131714
training loss: 2.57609224319458
training loss: 2.580735206604004
training loss: 2.527873992919922
training loss: 2.5686542987823486
training loss: 2.6186203956604004
training loss: 2.5646257400512695
training loss: 2.3260695934295654
training loss: 2.5798263549804688
training loss: 2.4663074016571045
training loss: 2.4792320728302
training loss: 2.5504322052001953
training loss: 2.495325803756714
training loss: 2.3119893074035645
training loss: 2.5437722206115723
training loss: 2.4214913845062256
training loss: 2.5650110244750977
training loss: 2.396723747253418
training loss: 2.5706429481506348
training loss: 2.620161771774292
training loss: 2.5670807361602783
training loss: 2.340179204940796
training loss: 2.460986852645874
training loss: 2.5875332355499268
training loss: 2.387209892272949
training loss: 2.456524133682251
training loss: 2.4113731384277344
training loss: 2.3758254051208496
training loss: 2.5633227825164795
training loss: 2.4745044708251953
training loss: 2.380612850189209
training loss: 2.574986219406128
training loss: 2.223003387451172
training loss: 2.4722952842712402
training loss: 2.4340035915374756
training loss: 2.3465917110443115
training loss: 2.5478968620300293
training loss: 2.453677177429199
training loss: 2.5380401611328125
training loss: 2.270991802215576
training loss: 2.5865120887756348
training loss: 2.456632614135742
training loss: 2.5421342849731445
training loss: 2.4332785606384277
training loss: 2.5539355278015137
training loss: 2.5722949504852295
training loss: 2.5566227436065674
training loss: 2.7075088024139404
training loss: 2.6380505561828613
training loss: 2.6122405529022217
training loss: 2.562279224395752
validation loss: 2.5202267169952393
training loss: 2.542590856552124
training loss: 2.582888603210449
training loss: 2.4180352687835693
training loss: 2.61893892288208
training loss: 2.3062424659729004
training loss: 2.5423316955566406
training loss: 2.5172605514526367
training loss: 2.4442689418792725
training loss: 2.3732078075408936
training loss: 2.5784826278686523
training loss: 2.409595012664795
training loss: 2.45729660987854
training loss: 2.468561887741089
training loss: 2.494029998779297
training loss: 2.462287425994873
training loss: 2.436558723449707
training loss: 2.480356216430664
training loss: 2.514716148376465
training loss: 2.479740858078003
training loss: 2.48720121383667
training loss: 2.455449342727661
training loss: 2.5695135593414307
training loss: 2.5537824630737305
training loss: 2.3505406379699707
training loss: 2.479616403579712
training loss: 2.382215976715088
training loss: 2.6268882751464844
training loss: 2.471789598464966
training loss: 2.4383249282836914
training loss: 2.311105966567993
training loss: 2.539245367050171
training loss: 2.4569015502929688
training loss: 2.404924154281616
training loss: 2.6207547187805176
training loss: 2.5947742462158203
training loss: 2.433769941329956
training loss: 2.613750457763672
training loss: 2.587071657180786
training loss: 2.4043831825256348
training loss: 2.5992088317871094
training loss: 2.5199966430664062
training loss: 2.618694305419922
training loss: 2.515040397644043
training loss: 2.3958349227905273
training loss: 2.481717109680176
training loss: 2.5458157062530518
training loss: 2.5276529788970947
training loss: 2.353510618209839
training loss: 2.5681915283203125
training loss: 2.6510138511657715
validation loss: 2.339323043823242
training loss: 2.5704944133758545
training loss: 2.59822678565979
training loss: 2.6245975494384766
training loss: 2.3939566612243652
training loss: 2.560765027999878
training loss: 2.5490801334381104
training loss: 2.6054468154907227
training loss: 2.593956470489502
training loss: 2.460482597351074
training loss: 2.6796274185180664
training loss: 2.481457233428955
training loss: 2.3516902923583984
training loss: 2.631767988204956
training loss: 2.540102243423462
training loss: 2.5840892791748047
training loss: 2.507990837097168
training loss: 2.5255625247955322
training loss: 2.317700147628784
training loss: 2.5771751403808594
training loss: 2.5059425830841064
training loss: 2.4614369869232178
training loss: 2.5447771549224854
training loss: 2.4460549354553223
training loss: 2.406898260116577
training loss: 2.5616161823272705
training loss: 2.5442113876342773
training loss: 2.5803122520446777
training loss: 2.435511827468872
training loss: 2.561709403991699
training loss: 2.3858025074005127
training loss: 2.536961793899536
training loss: 2.4049880504608154
training loss: 2.434624195098877
training loss: 2.576578378677368
training loss: 2.6024858951568604
training loss: 2.427724599838257
training loss: 2.318671703338623
training loss: 2.5831875801086426
training loss: 2.573803424835205
training loss: 2.508218288421631
training loss: 2.5512540340423584
training loss: 2.305663824081421
training loss: 2.5019190311431885
training loss: 2.5572493076324463
training loss: 2.557982921600342
training loss: 2.306201457977295
training loss: 2.53436541557312
training loss: 2.4923255443573
training loss: 2.5366125106811523
training loss: 2.686199426651001
validation loss: 2.432684898376465

Seed text: esa || LONEOS || NYS || align=right | 1.0 km ||  |-id=646 bgcolor=#fefefe | 158646 ||  || â || February 9, 2003 || Palomar || NEAT || â || align=right | 1.0 km ||  |-id=647 bgcolor=#fefefe | 158647 ||  || â || February 22, 2003 || Desert Eagle || W. K. Y. Yeung || â || align=right | 1.9 km ||  |-id=648 bgcolor=#fefefe | 158648 ||  || â || February 22, 2003 || Palomar || NEAT || V || align=right | 1.0 km ||  |-id=649 bgcolor=#fefefe | 158649 ||  || â || February 22, 2003 || Palomar || NEAT || NYS || align=right data-sort-value="0.97" | 970 m ||  |-id=650 bgcolor=#fefefe | 158650 ||  || â || February 22, 2003 || Palomar || NEAT || NYS || align=right | 1.0 km ||  |-id=651 bgcolor=#fefefe | 158651 ||  || â || February 26, 2003 || Campo Imperatore || CINEOS || NYS || align=right | 1.2 km ||  |-id=652 bgcolor=#fefefe | 158652 ||  || â || February 25, 2003 || Campo Imperatore || CINEOS || NYS || align=right data-sort-value="0.98" | 980 m ||  |-id=653 bgcolor=#fefefe | 158653 ||  || â || February 
****************************************************************************************************
Generated text: | ||| 1, be | || | | 29EAR  Sowalidewhts, || 158 | 181992.75 MLINE99EAR  | â NE903 alicoge || |- 19980â â | NEulight | LINEAAR | | |  | || Kaloliololo || || ||  |-igcolight || | | |  | | 27 7045
****************************************************************************************************
training loss: 2.6545164585113525
training loss: 2.367626190185547
training loss: 2.5326478481292725
training loss: 2.5592551231384277
training loss: 2.550261974334717
training loss: 2.5554165840148926
training loss: 2.4338202476501465
training loss: 2.6006455421447754
training loss: 2.3838469982147217
training loss: 2.603199005126953
training loss: 2.421550750732422
training loss: 2.5906646251678467
training loss: 2.5767405033111572
training loss: 2.5121684074401855
training loss: 2.596137523651123
training loss: 2.551619052886963
training loss: 2.5957624912261963
training loss: 2.4114272594451904
training loss: 2.5987441539764404
training loss: 2.542454481124878
training loss: 2.342395782470703
training loss: 2.3195693492889404
training loss: 2.507842540740967
training loss: 2.564927577972412
training loss: 2.4398627281188965
training loss: 2.48702073097229
training loss: 2.4341671466827393
training loss: 2.5451951026916504
training loss: 2.4514284133911133
training loss: 2.4416537284851074
training loss: 2.454117774963379
training loss: 2.4723150730133057
training loss: 2.4129631519317627
training loss: 2.4371132850646973
training loss: 2.218315601348877
training loss: 2.5791099071502686
training loss: 2.5427424907684326
training loss: 2.541391372680664
training loss: 2.6194567680358887
training loss: 2.524047374725342
training loss: 2.530348539352417
training loss: 2.520040512084961
training loss: 2.5702648162841797
training loss: 2.228487014770508
training loss: 2.3932173252105713
training loss: 2.5013346672058105
training loss: 2.6148154735565186
training loss: 2.488834857940674
training loss: 2.4877710342407227
training loss: 2.558616876602173
validation loss: 2.4471890926361084
training loss: 2.6126208305358887
training loss: 2.609023094177246
training loss: 2.4936468601226807
training loss: 2.5053412914276123
training loss: 2.557600498199463
training loss: 2.4597008228302
training loss: 2.6333980560302734
training loss: 2.5790040493011475
training loss: 2.584963083267212
training loss: 2.6756889820098877
training loss: 2.4567487239837646
training loss: 2.5692508220672607
training loss: 2.618220567703247
training loss: 2.550974130630493
training loss: 2.13283109664917
training loss: 2.586514949798584
training loss: 2.522430896759033
training loss: 2.50726056098938
training loss: 2.5486128330230713
training loss: 2.5768532752990723
training loss: 2.628516435623169
training loss: 2.6736786365509033
training loss: 2.363901138305664
training loss: 2.501638889312744
training loss: 2.164686679840088
training loss: 2.5906262397766113
training loss: 2.55375075340271
training loss: 2.4081361293792725
training loss: 2.574061632156372
training loss: 2.543720006942749
training loss: 2.4151809215545654
training loss: 2.711639165878296
training loss: 2.4432427883148193
training loss: 2.531771421432495
training loss: 2.401946544647217
training loss: 2.453375816345215
training loss: 2.438264846801758
training loss: 2.3407721519470215
training loss: 2.4404137134552
training loss: 2.5982208251953125
training loss: 2.5109434127807617
training loss: 2.4357101917266846
training loss: 2.504683256149292
training loss: 2.572476863861084
training loss: 2.6012728214263916
training loss: 2.447753667831421
training loss: 2.596362352371216
training loss: 2.4635119438171387
training loss: 2.2727909088134766
training loss: 2.5645246505737305
validation loss: 2.4687912464141846
training loss: 2.461880683898926
training loss: 2.4104700088500977
training loss: 2.505230188369751
training loss: 2.480086088180542
training loss: 2.4804036617279053
training loss: 2.512030601501465
training loss: 2.430820941925049
training loss: 2.5629665851593018
training loss: 2.5658650398254395
training loss: 2.5655007362365723
training loss: 2.427731513977051
training loss: 2.578026056289673
training loss: 2.3431429862976074
training loss: 2.548576593399048
training loss: 2.5460405349731445
training loss: 2.562800168991089
training loss: 2.6130619049072266
training loss: 2.580244779586792
training loss: 2.4924349784851074
training loss: 2.6418404579162598
training loss: 2.5563390254974365
training loss: 2.4613983631134033
training loss: 2.5650386810302734
training loss: 2.4750375747680664
training loss: 2.293452739715576
training loss: 2.497725486755371
training loss: 2.5124552249908447
training loss: 2.6879794597625732
training loss: 2.6325440406799316
training loss: 2.564539670944214
training loss: 2.58406925201416
training loss: 2.617969274520874
training loss: 2.60251522064209
training loss: 2.4509549140930176
training loss: 2.5358049869537354
training loss: 2.6306211948394775
training loss: 2.4601266384124756
training loss: 2.439121961593628
training loss: 2.5978426933288574
training loss: 2.435664653778076
training loss: 2.4513282775878906
training loss: 2.448153257369995
training loss: 2.4420742988586426
training loss: 2.565052032470703
training loss: 2.5526809692382812
training loss: 2.545405626296997
training loss: 2.3905107975006104
training loss: 2.553251266479492
training loss: 2.550586223602295
training loss: 2.596073627471924
validation loss: 2.552462577819824
training loss: 2.476557970046997
training loss: 2.508175849914551
training loss: 2.3158016204833984
training loss: 2.4141852855682373
training loss: 2.460730791091919
training loss: 2.600313186645508
training loss: 2.473829507827759
training loss: 2.2639541625976562
training loss: 2.559326171875
training loss: 2.5390002727508545
training loss: 2.588798999786377
training loss: 2.4520187377929688
training loss: 2.61194109916687
training loss: 2.5794684886932373
training loss: 2.4228241443634033
training loss: 2.522895574569702
training loss: 2.4582207202911377
training loss: 2.584346294403076
training loss: 2.5855727195739746
training loss: 2.59175705909729
training loss: 2.2813265323638916
training loss: 2.446819543838501
training loss: 2.4411046504974365
training loss: 2.611091136932373
training loss: 2.484651565551758
training loss: 2.551374912261963
training loss: 2.51373291015625
training loss: 2.3924007415771484
training loss: 2.4439473152160645
training loss: 2.5137884616851807
training loss: 2.5361573696136475
training loss: 2.4868979454040527
training loss: 2.448982000350952
training loss: 2.3374319076538086
training loss: 2.5647385120391846
training loss: 2.4250576496124268
training loss: 2.437736988067627
training loss: 2.551807165145874
training loss: 2.6450297832489014
training loss: 2.625749111175537
training loss: 2.5067217350006104
training loss: 2.4039318561553955
training loss: 2.416185140609741
training loss: 2.5573031902313232
training loss: 2.363278388977051
training loss: 2.608563184738159
training loss: 2.7369449138641357
training loss: 2.449951171875
training loss: 2.5039665699005127
training loss: 2.4275074005126953
validation loss: 2.5790674686431885
training loss: 2.330052375793457
training loss: 2.5319504737854004
training loss: 2.5791878700256348
training loss: 2.6045148372650146
training loss: 2.4878861904144287
training loss: 2.4380173683166504
training loss: 2.487739086151123
training loss: 2.41253924369812
training loss: 2.434481382369995
training loss: 2.327118158340454
training loss: 2.55911922454834
training loss: 2.5143654346466064
training loss: 2.46398663520813
training loss: 2.6099488735198975
training loss: 2.620152711868286
training loss: 2.522831678390503
training loss: 2.297269105911255
training loss: 2.416889190673828
training loss: 2.447451591491699
training loss: 2.57909893989563
training loss: 2.443579912185669
training loss: 2.5434114933013916
training loss: 2.6244990825653076
training loss: 2.5515363216400146
training loss: 2.271977424621582
training loss: 2.4585952758789062
training loss: 2.5962696075439453
training loss: 2.6093087196350098
training loss: 2.5405638217926025
training loss: 2.6097893714904785
training loss: 2.4472804069519043
training loss: 2.4367387294769287
training loss: 2.5552103519439697
training loss: 2.3250374794006348
training loss: 2.440707206726074
training loss: 2.562605619430542
training loss: 2.5465590953826904
training loss: 2.544264554977417
training loss: 2.550764799118042
training loss: 2.4583330154418945
training loss: 2.560784101486206
training loss: 2.2616732120513916
training loss: 2.5582892894744873
training loss: 2.5292139053344727
training loss: 2.4697604179382324
training loss: 2.3689327239990234
training loss: 2.56368350982666
training loss: 2.5238773822784424
training loss: 2.458937406539917
training loss: 2.550138473510742
validation loss: 2.639709949493408

Seed text: n=right data-sort-value="0.92" | 920 m ||  |-id=849 bgcolor=#fefefe | 156849 ||  || â || February 6, 2003 || Kitt Peak || Spacewatch || â || align=right data-sort-value="0.98" | 980 m ||  |-id=850 bgcolor=#fefefe | 156850 ||  || â || February 4, 2003 || Haleakala || NEAT || â || align=right data-sort-value="0.98" | 980 m ||  |-id=851 bgcolor=#fefefe | 156851 ||  || â || February 4, 2003 || Socorro || LINEAR || EUT || align=right data-sort-value="0.87" | 870 m ||  |-id=852 bgcolor=#fefefe | 156852 ||  || â || February 8, 2003 || Haleakala || NEAT || â || align=right | 1.5 km ||  |-id=853 bgcolor=#fefefe | 156853 ||  || â || February 7, 2003 || Desert Eagle || W. K. Y. Yeung || â || align=right | 1.2 km ||  |-id=854 bgcolor=#fefefe | 156854 ||  || â || February 8, 2003 || Socorro || LINEAR || â || align=right | 2.9 km ||  |-id=855 bgcolor=#fefefe | 156855 ||  || â || February 10, 2003 || Needville || Needville Obs. || NYS || align=right data-sort-value="0.97" | 970 m ||  |-id=856 bgcolor
****************************************************************************************************
Generated text: =#fefeerrriton=#E98, ||| 22  | |||  || || |-ign=r=#d=rrorid=rr | 20 | bgny 19E9EAC Ker, || 428, FLINEA.2636, | â1534 |  ||   ||| 176 || Spaghtoalokmalolove|| ||| | || ||||  | | â â || || 197| 1.
****************************************************************************************************
training loss: 2.4554762840270996
training loss: 2.435011863708496
training loss: 2.584312677383423
training loss: 2.5824685096740723
training loss: 2.5447003841400146
training loss: 2.530775308609009
training loss: 2.385315179824829
training loss: 2.5011043548583984
training loss: 2.5077502727508545
training loss: 2.494274377822876
training loss: 2.5687224864959717
training loss: 2.6579787731170654
training loss: 2.4515461921691895
training loss: 2.3163902759552
training loss: 2.4071543216705322
training loss: 2.5948686599731445
training loss: 2.3691020011901855
training loss: 2.512500524520874
training loss: 2.543574094772339
training loss: 2.600688934326172
training loss: 2.643277168273926
training loss: 2.4958581924438477
training loss: 2.4511122703552246
training loss: 2.5962562561035156
training loss: 2.4492294788360596
training loss: 2.2201011180877686
training loss: 2.4499363899230957
training loss: 2.5314204692840576
training loss: 2.5236268043518066
training loss: 2.5403823852539062
training loss: 2.6231582164764404
training loss: 2.396571636199951
training loss: 2.560076951980591
training loss: 2.207402467727661
training loss: 2.572483539581299
training loss: 2.248538017272949
training loss: 2.5531182289123535
training loss: 2.4763643741607666
training loss: 2.5237650871276855
training loss: 2.532595157623291
training loss: 2.3767895698547363
training loss: 2.552522897720337
training loss: 2.4090230464935303
training loss: 2.6658613681793213
training loss: 2.546665668487549
training loss: 2.4311888217926025
training loss: 2.467677116394043
training loss: 2.4942328929901123
training loss: 2.4068636894226074
training loss: 2.5517139434814453
validation loss: 2.3608670234680176
training loss: 2.5663869380950928
training loss: 2.4925050735473633
training loss: 2.393622398376465
training loss: 2.516507387161255
training loss: 2.5476343631744385
training loss: 2.4076945781707764
training loss: 2.548964023590088
training loss: 2.4444985389709473
training loss: 2.4125444889068604
training loss: 2.643554449081421
training loss: 2.578834295272827
training loss: 2.4267592430114746
training loss: 2.6014511585235596
training loss: 2.4664456844329834
training loss: 2.3962602615356445
training loss: 2.646855592727661
training loss: 2.5213463306427
training loss: 2.546111583709717
training loss: 2.3900375366210938
training loss: 2.4221785068511963
training loss: 2.4614779949188232
training loss: 2.4594566822052
training loss: 2.4151175022125244
training loss: 2.5358693599700928
training loss: 2.6023662090301514
training loss: 2.597393274307251
training loss: 2.4611105918884277
training loss: 2.572969913482666
training loss: 2.6030220985412598
training loss: 2.5680997371673584
training loss: 2.589068651199341
training loss: 2.567782163619995
training loss: 2.3668200969696045
training loss: 2.457834005355835
training loss: 2.533642053604126
training loss: 2.575157880783081
training loss: 2.602385997772217
training loss: 2.5798797607421875
training loss: 2.532883882522583
training loss: 2.5280027389526367
training loss: 2.4177210330963135
training loss: 2.5615267753601074
training loss: 2.3160157203674316
training loss: 2.5587425231933594
training loss: 2.5833020210266113
training loss: 2.5396623611450195
training loss: 2.423233985900879
training loss: 2.5712690353393555
training loss: 2.4491419792175293
training loss: 2.4197962284088135
validation loss: 2.2988991737365723
training loss: 2.4132182598114014
training loss: 2.568326950073242
training loss: 2.472489356994629
training loss: 2.5836095809936523
training loss: 2.557603597640991
training loss: 2.3884031772613525
training loss: 2.4314892292022705
training loss: 2.426983594894409
training loss: 2.563720941543579
training loss: 2.535024642944336
training loss: 2.2757408618927
training loss: 2.525991916656494
training loss: 2.4758362770080566
training loss: 2.413752317428589
training loss: 2.61087703704834
training loss: 2.36312198638916
training loss: 2.5743284225463867
training loss: 2.391749143600464
training loss: 2.460989475250244
training loss: 2.408001661300659
training loss: 2.248929023742676